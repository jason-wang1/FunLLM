# 语言模型简介
## 机器理解人类语言的挑战

人类自然语言的多样性、灵活性、歧义性、上下文依赖性、语言的变化以及世界知识和常识的应用等因素都使得让机器难以理解人的自然语言：

1. 多样性和灵活性：包括语法、词汇、语义、上下文等方面。同一个词汇在不同语境中可能有不同的含义，例如“他被杀死了”的“死”和“笑死我了”的死完全是两个含义。人们还经常使用讽刺、隐喻、比喻、口语和俚语等非字面意义的表达方式，比如“潘帕斯雄鹰真是这场盛典中的一匹黑马”，潘帕斯雄鹰指的是阿根廷队而不是一个动物，盛典指的是世界杯，黑马是比喻阿根廷队取得了超乎我们预期的表现。人是可以理解这些语言的，但机器想理解这些就非常困难。
2. 语言的歧义性：自然语言中存在丰富的歧义现象，包括词义歧义、语法歧义、指代歧义等。例如，英语中的词汇"bank"可以表示银行，也可以表示河岸，其含义取决于上下文，这是词语歧义；The animal didn't cross the street because it was too tired，对于机器来说，这里的it可以指代animal，也可以指代street。这是一种指代歧义。这种歧义性对于机器来说是一个挑战，因为需要具有推理和语境理解能力才能够正确解析歧义。
3. 上下文依赖性：人类自然语言中的表达通常依赖于上下文信息。例如，尽管大多数时候terribly的意思是负面的，但The movie is terribly exciting中，terribly的意义就是积极的。
4. 世界知识和常识：人类在语言交流中经常依赖于世界知识和常识，即我们对世界的认知和经验。当我们说"我要花30元去伦敦看电影"时，我们知道电影是什么，知道伦敦是个地名，知道30是个数字而元是货币单位。这种世界知识和常识对于理解自然语言是至关重要的，但对于机器来说，获取和应用这些知识是一项复杂的任务。

## 语言模型 Language Model 定义

在不同的文章和材料中会看到对语言模型的不同定义，这可能会让人有些疑惑，不过下面我们都会讲清楚的。

### 估计一个给定词序列在语言上的合理性或者说概率

首先，对于任意的词序列，语言模型能够用计算出这个词序列是一句话的概率。

例如，对于: $$\left\{ \begin{aligned}s_1:武汉｜的｜天气｜真｜差\\ s_2：天气｜北京｜的｜差｜真 \\ \end{aligned} \right. $$，应该要有: $$\left\{ \begin{aligned} P(S_1）=1（或接近于1）\\ P(S_2）=0（或接近于0）\\ \end{aligned} \right. $$。

这其实是语言模型最早的功能，它的起源其实是Speech Recognition，即语音转文字！从音频转到文字的时候，会有很多个句子作为候选。那么哪个句子更合理？这种时候我们就可以用语言模型，对这些句子Make Sense的概率做一个排序。**这时候，对于语言模型的定义为：**语言模型（Language Model）是一种用于生成自然语言文本的概率模型。它可以估计一个给定文本序列（通常是一个句子或者一个短语）在语言上的合理性或者说概率：

给定一个词典$$V$$，可以计算对于任意单词$$w_i\in V$$，词序列$$w_1,w_2,w_3,....,w_n$$是一句Make Sense的句子的概率：

$$P(S)=P（w_1,w_2,w_3,....,w_n）$$

### 根据先前的文本序列预测下一个词，从而实现文本生成

当然，现在我们知道语言模型真正在做的事情是下一个词的预测和输出了。

> 最常见的的语言模型大概就是手机输入法，它能根据你当前输入的内容提示下一个字或者词。

不过这其实和上面的定义是一脉相承的。如果我们能预测词序列$$w_1,w_2,w_3,....,w_n$$是一句Make Sense的句子的概率，那么我们自然也可以去预测词序列$$w_1,w_2,w_3,....,w_n,w_{n+1}$$是一句Make Sense的话的概率。这样的话，已知$$w_1,w_2,w_3,....,w_n$$，不就可以去选择一个$$w_{n+1}$$了吗？

**我们这么补充语言模型的定义：**语言模型可以根据先前的文本序列预测下一个可能出现的词或者字符，从而生成新的文本。如果是以已有的全部词序列为预测依据，那么，语言模型就是在求：$$P(w_{i}|w_{1},w_{2},...,w_{i-1};\theta)$$

那么，语言模型的目标函数（损失函数）就变成了一个我们非常熟悉的东西（2.2.2.1）：

$$\mathcal L(\mathcal U)=-∑^i_1logP(w_{i}|w_{1},w_{2},...,w_{i-1};\theta)$$

*还记得交叉熵损失函数吗？它就是模型理应输出1的那一维向量的元素的负对数。*

其中，$$\mathcal U$$是巨大无比的**语料库（Corpus）**，$$ w$$是一个个的词语，$$\theta$$是模型的参数。我们训练语言模型的目的，就是在于调整参数$$\theta$$，让模型接受这段话前面的词语们，尽量根据语料库，输出下一个词语$$w_i$$。

所以，本质上语言模型就是一个基于概率的自回归填字游戏。

### 我们期待语言模型能有怎样的表现？

现在我们用V表示一个词典：$$V=\begin{bmatrix} 猫&狗&在&卧室&书房&吃&肉&鱼&黑&白&黄&紫 \end{bmatrix}$$，那么：

例如，对于: $$\left\{ \begin{aligned}s_1:猫｜在｜卧室｜吃｜鱼\\ s_2：狗｜在｜书房｜吃｜肉 \\ \end{aligned} \right. $$,应该要有: $$P(s_1)\approx P(s_2)$$，因为猫和狗是相似的，卧室和书房是相似的，鱼和肉也是相似的。

**我们希望语言模型有足够的泛化能力。**

对于: $$\left\{ \begin{aligned}s_1:猫｜在｜卧室｜吃｜鱼\\ s_2：猫｜在｜书房｜吃｜鱼 \\ \end{aligned} \right. $$，我们当然还是希望: $$P(s_1)\approx P(s_2)$$，因为没人规定猫只能在卧室，不能在书房。

对于: $$\left\{ \begin{aligned}s_1:黑｜猫\\ s_2：白｜猫 \\ \end{aligned} \right. $$，我们自然也希望: $$P(s_1)\approx P(s_2)$$，因为猫确实可以有很多颜色。

**但是我们也不希望语言模型太过于“灵活”发散：**

对于: $$\left\{ \begin{aligned}s_1:猫｜在｜卧室｜吃｜鱼\\ s_2：狗｜在｜书房｜吃｜鱼 \\ \end{aligned} \right. $$，我们就不希望: $$P(s_1)\approx P(s_2)$$了！！因为狗真的不至于吃鱼！*（有人跟我说他们家的狗就是爱吃鱼，但是无所谓了！！）*

对于: $$\left\{ \begin{aligned}s_1:黑｜猫\\ s_2：紫｜猫 \\ \end{aligned} \right. $$，我们就不希望: $$P(s_1)\approx P(s_2)$$了！！因为猫真的不大可能是紫色！

**我们希望语言模型可以根据上下文发现词汇之间是相似的。**

- 如果“NLP”和“自然语言处理”在语料中的上下文很像，那么语言模型就应该能发现，NLP和自然语言处理是一个意思。人类很容易就可以理解NLP就是自然语言处理，但是对于机器来说，这必须得学习自他们相似的上下文。
- 同样的，如果我们已知了NLP和自然语言处理这两个词是一样的，那么它们的上下文应该也是相似的。

我们还可以做一些有意思的类比实验来测试语言模型的能力。

例如，如果我们用了词嵌入的方式来表示词汇，那么，应该有：

$$\vec{国王}-\vec{王后}\approx\vec{男人}-\vec{女人}$$

*（嗯，这是word2vec刚出来的时候很震撼的原因之一，让人觉得woc还能这么玩——）*













